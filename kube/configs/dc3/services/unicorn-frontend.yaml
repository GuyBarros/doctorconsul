apiVersion: v1
kind: ServiceAccount
metadata:
  name: unicorn-frontend
  namespace: unicorn

---

apiVersion: consul.hashicorp.com/v1alpha1
kind: ServiceDefaults
metadata:
  name: unicorn-frontend
  namespace: unicorn
spec:
  protocol: http

---

apiVersion: v1
kind: Service
metadata:
  name: unicorn-frontend
  namespace: unicorn
spec:
  type: LoadBalancer        # This may not work with k3s, since this was taken from an AKS config
  selector:
    app: unicorn-frontend
  ports:
    - name: http
      protocol: TCP
      port: 8000            # Should be the port the Kube LB will listen on to forward to TCP/10000
      targetPort: 10000     # This should line up with the FakeService LISTEN_ADDR: 0.0.0.0:10000

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: unicorn-frontend
  namespace: unicorn
  labels:
    app: unicorn-frontend
    version: v1
    # service: fake-service      # do I need this?
spec:
  replicas: 1
  selector:
    matchLabels:
      app: unicorn-frontend
  template:
    metadata:
      labels:
        app: unicorn-frontend
        # service: fake-service     # do I need this?
      annotations:
        consul.hashicorp.com/connect-inject: 'true'
        consul.hashicorp.com/transparent-proxy: 'true'
        consul.hashicorp.com/service-tags: 'dc3'
        # consul.hashicorp.com/service-meta-version: 'v1'
        # consul.hashicorp.com/connect-service-upstreams: "unicorn-backend.unicorn:11000:dc3"
        # consul.hashicorp.com/connect-service-upstreams: "unicorn-backend.unicorn.cernunnos:12000"
        # consul.hashicorp.com/connect-service-upstreams: "unicorn-backend.unicorn.cernunnos:12000:dc3"
        # consul.hashicorp.com/connect-service-upstreams: "unicorn-backend.unicorn.default:11000:dc3"
        # consul.hashicorp.com/connect-service-upstreams: "unicorn-backend.unicorn:11000:dc3,unicorn-backend.unicorn.cernunnos:12000"
        consul.hashicorp.com/connect-service-upstreams: "unicorn-backend.unicorn:11000:dc3,unicorn-backend.unicorn.cernunnos:12000,unicorn-backend.svc.unicorn.ns.dc4-default.peer:13000,unicorn-backend.svc.unicorn.ns.dc4-taranis.peer:14000"
        consul.hashicorp.com/transparent-proxy-exclude-inbound-ports: "10000"   # Without this exclusion the FakeService UI is shitcanned.
    spec:
      serviceAccountName: unicorn-frontend
      containers:
        - name: unicorn-frontend
          image: nicholasjackson/fake-service:v0.25.0
          # imagePullPolicy: Always       # Probably don't need this
          ports:
            - containerPort: 10000
          # readinessProbe:
          #   httpGet:
          #     scheme: HTTP
          #     path: /
          #     port: 10000
            # initialDelaySeconds: 10
            # periodSeconds: 5
          env:
            - name: 'LISTEN_ADDR'
              value: '0.0.0.0:10000'
            - name: 'UPSTREAM_URIS'
              value: 'http://127.0.0.1:11000,http://127.0.0.1:12000,http://127.0.0.1:13000,http://127.0.0.1:14000,http://unicorn-tp-backend.virtual.unicorn.ns.dc3.dc.consul,http://unicorn-tp-backend.virtual.unicorn.ns.cernunnos.ap.dc3.dc.consul,http://unicorn-tp-backend.virtual.unicorn.dc4-default.consul,http://unicorn-tp-backend.virtual.unicorn.dc4-taranis.consul'
              # value: 'http://127.0.0.1:11000,http://127.0.0.1:12000,http://unicorn-tp-backend.virtual.unicorn.ns.dc3.dc.consul,http://unicorn-tp-backend.virtual.unicorn.ns.cernunnos.ap.dc3.dc.consul'
            - name: 'NAME'
              value: 'unicorn-frontend (DC3)'
            - name: 'MESSAGE'
              value: '<p><strong>The Unicorn-frontend Application</strong></p><p>The&nbsp;DC3 Unicorn-frontend application has the following features:</p><ul><li>8 total HTTP upstream services<ul><li>4 explicitly configured<ul><li>TCP/11000: dc3 / default (ap) / unicorn / unicorn-backend</li><li>TCP/12000: dc3 / cernunnos (ap) / unicorn / unicorn-backend</li><li>TCP/13000: dc4-default (peer) / unicorn / unicorn-backend</li><li>TCP/14000: dc4-taranis (peer) / unicorn / unicorn-backend</li></ul></li><li>4 transparently accessed<ul><li>http://unicorn-tp-backend.virtual.unicorn.ns.dc3.dc.consul</li><li>http://unicorn-tp-backend.virtual.unicorn.ns.cernunnos.ap.dc3.dc.consul</li><li>http://unicorn-tp-backend.virtual.unicorn.dc4-default.consul</li><li>http://unicorn-tp-backend.virtual.unicorn.dc4-taranis.consul</li></ul></li></ul></li><li>The dc3 / unicorn / unicorn-tp-backend service belongs to a Sameness Group (service-resolver), which uses the following order:<ul><li>dc3 default</li><li>dc3 cernunnos (AP)</li><li>dc4 default (Peer)</li><li>dc4 taranis (Peer)</li></ul></li><li>2 external upstreams accessible via Terminating Gateway:<ul><li>&quot;example-https&quot;<ul><li>example.com:443</li><li>wolfmansound.com:443</li></ul></li><li>&quot;whatismyip&quot;<ul><li>104.16.154.36:443</li></ul></li></ul></li></ul><p><strong>Noteworthy Details</strong></p><p>This application has a little bit of everything in it. Many different upstreams of both explicit and transparent types, service failover, and external service via a terminating gateway.&nbsp;</p><p>Each transparently accessed upstream service is named with a {transparent} tag. This makes it easy to differentiate from the explicitly access services.</p><p><strong>Demo: Failover on dc3 / unicorn / unicorn-tp-backend</strong></p><p>Every time you refresh the Fake Service UI, a new request is made to each upstream service. Pay close attention to the service named &quot;unicorn-backend {transparent} (DC3)&quot; service. As we one by one destroy upstream services, you will watch in real-time as Consul switches to healthy unicorn-backend services in other locations.&nbsp;</p><p>Doctor Consul provides a k9s plugin to assist with scaling pods. See the zork script.</p><ul><li>In Kube cluster k3d-dc3, scale unicorn-tp-backend to 0 pods.</li><li>Refresh Fake Service</li></ul><p>Notice that there are now 2 instances of unicorn-backend {transparent} DC3 Cernunnos now. This is because the instance in DC3 is no longer healthy and has switched to the Cernunnos partition.</p><ul><li>In Kube cluster k3d-dc3-p1, scale unicorn-tp-backend to 0 pods.</li><li>Refresh Fake Service</li></ul><p>Notice that upstream service has now switched to unicorn-backend {transparent} DC4 and there are two occurrences of it. Also notice that the original cernunnos upstream is now red. This is because it is down and there is no service-resolver or sameness group that instructs Consul to failover to a healthy backup. Only the unicorn-tp-backend in DC3/default has failover enabled.&nbsp;</p><ul><li>In Kube cluster k3d-dc4, scale unicorn-tp-backend to 0 pods.</li><li>Refresh Fake Service</li></ul><p>Notice that once again&nbsp;the upstream destination has changed, now to DC4 Taranis.</p><ul><li>In Kube cluster k3d-dc4-p1, scale unicorn-tp-backend to 0 pods.</li><li>Refresh Fake Service</li></ul><p>Finally, since all instances of unicorn-tp-backend have been killed, we have run out of healthy upstreams and Fake Service has all red failures for the transparent upstreams.&nbsp;</p>'
            - name: 'SERVER_TYPE'
              value: 'http'
            - name: 'TIMING_50_PERCENTILE'
              value: '30ms'
            - name: 'TIMING_90_PERCENTILE'
              value: '60ms'
            - name: 'TIMING_99_PERCENTILE'
              value: '90ms'
            - name: 'TIMING_VARIANCE'
              value: '10'
            # - name: 'HTTP_CLIENT_APPEND_REQUEST'
            #   value: 'true'
            # - name: 'TRACING_ZIPKIN'
            #   value: 'http://simplest-collector.default:9411'
